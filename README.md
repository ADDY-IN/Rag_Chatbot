# âš“ Naval Intelligence RAG Chatbot â€” Prototype

An AI-powered Retrieval-Augmented Generation (RAG) chatbot built using **LangChain**, **Ollama**, and **Streamlit**.  
This prototype processes large-scale **naval documentation** (ships, submarines, and technical PDFs) to provide precise, context-based answers â€” directly extracted from your uploaded or indexed files.

---

## ğŸš€ Features

- âš™ï¸ **RAG Pipeline** â€” Combines document embeddings + vector retrieval + LLM inference.  
- ğŸ“š **Multi-Folder Document Indexing** â€” Scans and indexes all PDFs inside the `data/` folder (including nested directories).  
- ğŸ§© **Embeddings via Ollama** â€” Uses `nomic-embed-text` model for local embeddings (falls back to HuggingFace if unavailable).  
- ğŸ§  **Local LLM Integration** â€” Works with `phi3`, `llava`, or any other Ollama-supported model.  
- ğŸ–¼ï¸ **Image Support Ready** â€” Extendable for multimodal (text + image) documents.  
- ğŸŒ **Streamlit UI** â€” Interactive and lightweight prototype dashboard.

---

## ğŸ§© Tech Stack

| Component | Description |
|------------|-------------|
| **LangChain** | Framework for chaining LLM + retriever logic |
| **Ollama** | Local language model and embedding host |
| **Streamlit** | Web UI framework |
| **ChromaDB** | Vector store for efficient retrieval |
| **HuggingFace** | Backup embedding generator (MiniLM) |

---

## ğŸ“ Project Structure

Rag_Chatbot_Project/
â”‚
â”œâ”€â”€ main.py                 # Streamlit app and RAG logic
â”œâ”€â”€ requirements.txt        # Dependencies list
â”œâ”€â”€ data/                   # Naval PDFs and documents
â”œâ”€â”€ vectorstore/            # Chroma vector database
â”œâ”€â”€ .env                    # Optional environment file
â””â”€â”€ README.md               # Project overview

---

## âš™ï¸ Installation & Setup

### 1ï¸âƒ£ Clone the Repository
```bash
git clone https://github.com/YOUR_USERNAME/Rag_Chatbot_Project.git
cd Rag_Chatbot_Project

2ï¸âƒ£ Create Virtual Environment

python3 -m venv venv
source venv/bin/activate

3ï¸âƒ£ Install Requirements

pip install -r requirements.txt

4ï¸âƒ£ Start Ollama

Make sure Ollama is installed and running:

ollama serve
ollama pull phi3
ollama pull nomic-embed-text

5ï¸âƒ£ Run the App

streamlit run main.py

â¸»

ğŸ”’ Notes
	â€¢	This prototype is designed for local testing and client demo.
	â€¢	For production use:
	â€¢	Host embeddings and models on a secure cloud backend.
	â€¢	Implement caching, async queries, and improved retrievers.
	â€¢	Add authentication and logging for enterprise use.

â¸»

ğŸ§‘â€ğŸ’» Author

Aditya Kaushik
ğŸ“ Built for client demo â€” Naval RAG Chatbot (2025)
ğŸ’¬ Powered by Ollama, LangChain, and Streamlit
